{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa342d4-45df-43eb-9b19-8102399c36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### 1.IMPORT REQUIRED LIBRARYS\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    " pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "###  2.READING THE DATA \n",
    "\n",
    "def get_data():\n",
    "\n",
    "    #Noida  city csv file:\n",
    "    #jaipur=pd.read_csv('jaipur.csv')\n",
    "    \n",
    "    #Noida  city csv file:\n",
    "    #bhopal=pd.read_csv('bhopal.csv')\n",
    "\n",
    "    \n",
    "    #Noida  city csv file:\n",
    "    #vadodara=pd.read_csv('vadodara.csv')\n",
    "\n",
    "    \n",
    "    #Noida  city csv file:\n",
    "    #surat=pd.read_csv('surat.csv')\n",
    "\n",
    "    \n",
    "    #Noida  city csv file:\n",
    "    #kanpur=pd.read_csv('kanpur.csv')\n",
    "\n",
    "    \n",
    "    #Noida  city csv file:\n",
    "    #Guwahati=pd.read_csv('Guwahati.csv')\n",
    "\n",
    "     #Noida  city csv file:\n",
    "    #Faridabad=pd.read_csv('Faridabad.csv')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    #Noida  city csv file:\n",
    "    #Lucknow=pd.read_csv('Lucknow.csv')\n",
    "\n",
    "    #indore  city csv file:\n",
    "    #indore=pd.read_csv('indore.csv')\n",
    "\n",
    "    #Noida  city csv file:\n",
    "    Noida=pd.read_csv('Noida.csv')\n",
    "\n",
    "    #Ghaziyabad  city csv file:\n",
    "    #Ghaziyabad=pd.read_csv('Ghaziyabad.csv')\n",
    "\n",
    "    # Ahmadabad city csv file:\n",
    "    Ahmadabad=pd.read_csv('Ahmadabad.csv')\n",
    "    \n",
    "\n",
    "    # kolkata city csv file:\n",
    "    kolkata=pd.read_csv('Kolkata.csv')\n",
    "\n",
    "    \n",
    "    # Hydrabadcity csv file:\n",
    "    hyderabad=pd.read_csv('Hydrabad.csv')\n",
    "    \n",
    "    \n",
    "    # gurgaoncity csv file:\n",
    "    gurgaon=pd.read_csv('Gurgaon.csv')\n",
    "\n",
    "    # Delhi city csv file:\n",
    "    Delhi=pd.read_csv('Delhi.csv')\n",
    "    \n",
    "    # Chennai city csv file:\n",
    "    Chennai=pd.read_csv('Chennai.csv')\n",
    "\n",
    "    # Banglore city csv file:\n",
    "    Banglore=pd.read_csv('Banglore.csv')\n",
    "    \n",
    "    # pune city csv file:\n",
    "    pune=pd.read_csv('Pune.csv')\n",
    "    \n",
    "    # mumbai city csv file:\n",
    "    mumbai=pd.read_csv('Mumbai.csv')\n",
    "\n",
    "   ### merge alll above city into one dataframe:\n",
    "    return  pd.concat([mumbai,pune,Banglore,Chennai,Delhi,gurgaon,hyderabad,kolkata,Ahmadabad,Noida],ignore_index=True)\n",
    "    #return  pd.concat([mumbai,pune,Banglore,Chennai,Delhi,gurgaon,hyderabad,kolkata,Ahmadabad,Ghaziyabad,Noida,indore,Lucknow,jaipur,bhopal,vadodara,surat,kanpur,Guwahati,Faridabad],ignore_index=True\n",
    "    \n",
    "    \n",
    "\n",
    "data=get_data()\n",
    "\n",
    "data\n",
    "\n",
    "data.info()\n",
    "\n",
    "-   The data set contains 82,016 rows and 10 features\n",
    "-   column \"Bathrooms ,Facing,Appartment_Name,\" have lots of  missing values\n",
    "-   The data type of some features are not appropriate\n",
    "\n",
    "### 3.PREMLINARY DATA ANALYSIS\n",
    "\n",
    "#### 3.1Check for datatype\n",
    "\n",
    "\n",
    "data.sample(150)\n",
    "\n",
    "data.dtypes\n",
    "\n",
    "#### 3.2Check For Duplicates\n",
    "\n",
    "# problem statement \n",
    "\n",
    "data.duplicated().sum()\n",
    "\n",
    "duplicated_rows=(\n",
    "    data.loc[data.duplicated(keep=False)].sort_values([\"Rent\",\"Area\",\"appartment_type\",\"Bathrooms\",\"Facing\",\"Address\",\"Apartment Name\",\"Status\"])\n",
    ")\n",
    "\n",
    "duplicated_rows.sample(150)\n",
    "\n",
    "duplicated_rows.shape\n",
    "\n",
    "82016-68994\n",
    "\n",
    "#### 3.3 Observations\n",
    "\n",
    "- The type of `Rent`, `Deposit`, `bathroom`should be changed to numeric type.\n",
    "- There're 6700 duplicates. These should be removed.\n",
    "\n",
    "### 4.DETAILED ANALYSIS\n",
    "\n",
    "#### broker_consultancy\n",
    "\n",
    "\n",
    "data.broker_consultancy\n",
    "\n",
    "data.broker_consultancy.dtypes\n",
    "\n",
    "data.broker_consultancy.isna().sum()\n",
    "\n",
    "data.broker_consultancy.unique()\n",
    "\n",
    "-   The data type of broker_consultancy is valid\n",
    "-   broker_consultancy contain one null value\n",
    "-   There is no used of this feature in model_trainng\n",
    "-   we are going drop this in data_cleaning operation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Rent\n",
    "\n",
    "data.Rent\n",
    "\n",
    "data.Rent.isna().sum()\n",
    "\n",
    "data.Rent.dtypes\n",
    "\n",
    "-   The data type of Rent  is not valid\n",
    "-   In rent column data contain alphanumeric values like 1.5L,3.5L values\n",
    "-   we need to convert this into the numeric value\n",
    "-   This feature contain zero null value\n",
    "\n",
    "\n",
    "\n",
    "#### Area\n",
    "\n",
    "data.Area.dtypes\n",
    "\n",
    "data.Area.unique()\n",
    "\n",
    "data.Area.value_counts()\n",
    "\n",
    "-   The data type of Rent  is  valid\n",
    "-   Does not contains any null value\n",
    "-   it content many outlier values\n",
    "\n",
    "\n",
    "#### Appartment_type\n",
    "\n",
    "data.appartment_type.dtypes\n",
    "\n",
    "data.appartment_type.isna().sum()\n",
    "\n",
    "data.appartment_type.value_counts()\n",
    "\n",
    "-   The data type of Rent  is  valid\n",
    "-   Does not contains any null value\n",
    "-   we are going to remove the apparttment type which has ocuurance  less than 150 \n",
    "\n",
    "data.Deposit.unique()\n",
    "\n",
    "\n",
    "(\n",
    "    data\n",
    "    .Deposit\n",
    "    .loc[lambda ser: ser.str.contains(\"[^No Deposit]\")]\n",
    "     .count()\n",
    "    \n",
    ")\n",
    "\n",
    "-   The data type of Deposit   is not  valid\n",
    "-   This feature have lots of value as \"no_Deposit\"\n",
    "-   because of that we are going to drop this feature\n",
    "\n",
    "#### Bathrooms\t\n",
    "\n",
    "data.Bathrooms\t\n",
    "\n",
    "data.Bathrooms.isna().sum()\n",
    "\n",
    "(\n",
    "    data\n",
    "    .Bathrooms\n",
    "     .str.strip()\n",
    "     .str.get(0)\n",
    "      \n",
    "     \n",
    "     \n",
    "    \n",
    ")\n",
    "\n",
    "-   The data type of Bathrooms is not  valid\n",
    "-   it should be numeric\n",
    "-   this feature contains 344 null value\n",
    "-   in datacleaning operation the above problems are handle\n",
    "\n",
    "#### Facing\n",
    "\n",
    "\n",
    "-   This feature have lots of null value\n",
    "-   because of that we are going to drop this feature\n",
    "\n",
    "#### Address\n",
    "\n",
    "data.Address\n",
    "\n",
    "data.Address.isna().sum()\n",
    "\n",
    "data['Address'].str.split(',').apply(lambda x:x[1]).unique()\n",
    "\n",
    "len(data['Address'].str.split(',').apply(lambda x:x[1]).unique())\n",
    "\n",
    "len(data['Address'].str.split(',').apply(lambda x:x[0]).unique())\n",
    "\n",
    "\n",
    "-   This  Address feature have loction and city \n",
    "-   we have requirement of city name\n",
    "-   because of that we have to seprate out the locality and city from address\n",
    "-   add new feature as city and drop the feature address from dataframe\n",
    "-   This feature does not contains missing values \n",
    "\n",
    "#### Apartment Name \n",
    "\n",
    "-   This feature have lots of null value\n",
    "-   because of that we are going to drop this feature\n",
    "\n",
    "#### Status\n",
    "\n",
    "data.Status\n",
    "\n",
    "data.Status.isna().sum()\n",
    "\n",
    "data.Status.unique()\n",
    "\n",
    "-   This Feature has valid datatype\n",
    "-   Does not contains any null value\n",
    "-   not required any cleaning opertion\n",
    "\n",
    "#### Rent/sqfit\n",
    "\n",
    "-   There is no use of this feature \n",
    "-   because of that we are going to drop this feature\n",
    "\n",
    "### Locality_class\n",
    "\n",
    "data.localityclass.isna().sum()\n",
    "\n",
    "\n",
    "-   This Feature has valid datatype\n",
    "-   Does not contains any null value\n",
    "-   not required any cleaning opertion\n",
    "\n",
    "### 5.DATA CLEANING OPERATION\n",
    "\n",
    "\n",
    " def convert_rent(rent):\n",
    "    if pd.isna(rent):  # Check for NaN values\n",
    "        return np.nan\n",
    "    \n",
    "    rent = str(rent)  # Ensure rent is treated as a string\n",
    "    \n",
    "    if \"L\" in rent:\n",
    "        return int(float(rent.replace('L', '').replace(',', '').strip()) * 100000)\n",
    "    else:\n",
    "        return int(float(rent.replace(',', '').strip()))\n",
    "\n",
    "def clean_data(df):\n",
    "    # Drop the 'broker_consultancy' column and rename columns to lowercase\n",
    "    df_cleaned = (\n",
    "        df\n",
    "         \n",
    "        .drop(columns=\"broker_consultancy\")  # Drop the 'broker_consultancy','Deposit' column\n",
    "        .drop(columns=\"Deposit\")\n",
    "        .drop(columns=\"Facing\")\n",
    "        .drop(columns=\"Apartment Name\")\n",
    "        .drop(columns=\"Rent/sqfit\")\n",
    "        \n",
    "        #.drop_duplicates()\n",
    "        \n",
    "        .rename(columns=str.lower)            # Rename columns to lowercase\n",
    "        .assign\n",
    "        ( bathrooms=lambda df_:\n",
    "         (\n",
    "            df_\n",
    "            \n",
    "         .bathrooms\n",
    "         .str.strip()\n",
    "          .str.get(0)   \n",
    "             \n",
    "         )\n",
    "            \n",
    "        )\n",
    "   \n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    # Apply the convert_rent function to the 'rent' column\n",
    "    df_cleaned['rent'] = df_cleaned['rent'].apply(convert_rent)\n",
    "\n",
    "    if 'bathrooms' in df_cleaned.columns:\n",
    "        mode_bathrooms = df_cleaned['bathrooms'].mode()[0]  # Calculate mode\n",
    "        df_cleaned['bathrooms'] = df_cleaned['bathrooms'].fillna(mode_bathrooms) \n",
    "\n",
    "    # Convert 'bathrooms' column to integer type\n",
    "    df_cleaned['bathrooms'] = df_cleaned['bathrooms'].astype(int)\n",
    "\n",
    "    df_cleaned['city']=df_cleaned['address'].str.split(',').apply(lambda x:x[1])\n",
    "    #df_cleaned['locality']=df_cleaned['address'].str.split(',').apply(lambda x:x[0])\n",
    "    \n",
    "    df_cleaned = df_cleaned.drop(columns=\"address\")\n",
    "    \n",
    "    # Rename columns by removing or replacing substrings\n",
    "    #df_cleaned.columns = df_cleaned.columns.str.replace('area', '')\n",
    "    #df_cleaned.columns = df_cleaned.columns.str.replace('builtup', 'size')\n",
    "    df_cleaned.columns = df_cleaned.columns.str.replace('appartment_type', 'type')\n",
    "    df_cleaned.columns = df_cleaned.columns.str.replace('status', 'furnished_type')\n",
    "\n",
    "    if 'type' in df_cleaned.columns:\n",
    "        value_counts = df_cleaned['type'].value_counts()\n",
    "        types_to_keep = value_counts[value_counts >= 150].index\n",
    "        df_cleaned = df_cleaned[df_cleaned['type'].isin(types_to_keep)]\n",
    "\n",
    "    df_cleaned['city'] =  df_cleaned['city'].str.strip()\n",
    "    df_cleaned['city'] = df_cleaned['city'].replace('Thane west', 'Mumbai')\n",
    "    df_cleaned['city'] = df_cleaned['city'].replace('','Mumbai')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return df_cleaned\n",
    "    \n",
    "\n",
    "\n",
    "cleandata=clean_data(data)\n",
    "\n",
    "\n",
    "cleandata.info()\n",
    "\n",
    "cleandata.describe()\n",
    "\n",
    "cleandata.sample(150)\n",
    "\n",
    "len(cleandata.city.unique())\n",
    "\n",
    "cleandata.city.value_counts()\n",
    "\n",
    "cleandata.type.value_counts()\n",
    "\n",
    "cleandata.localityclass.value_counts()\n",
    "\n",
    "### 6.OUTLIERS DETECTIONS\n",
    "\n",
    "\n",
    "def find_outliers(df, column_name):\n",
    "    \"\"\"\n",
    "    Identifies outliers in the specified column of the DataFrame using the IQR method.\n",
    "    \"\"\"\n",
    "    # Check if the column exists in the DataFrame\n",
    "    if column_name not in df.columns:\n",
    "        raise KeyError(f\"Column '{column_name}' does not exist in the DataFrame.\")\n",
    "    \n",
    "    # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "    Q1 = df[column_name].quantile(0.25)\n",
    "    Q3 = df[column_name].quantile(0.75)\n",
    "    # Calculate IQR\n",
    "    IQR = Q3 - Q1\n",
    "    # Determine outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    # Return boolean mask for outliers\n",
    "    return (df[column_name] < lower_bound) | (df[column_name] > upper_bound)\n",
    "\n",
    "\n",
    "#### Rent\n",
    "\n",
    "\n",
    "# Find outliers for 'rent'\n",
    "rent_outliers = find_outliers(cleandata, 'rent')\n",
    "print(\"Outliers for 'rent':\")\n",
    "print(rent_outliers)\n",
    "\n",
    "def drop_outliers(df, column_name):\n",
    "    \"\"\"\n",
    "    Drops outliers from the specified column of the DataFrame and returns the cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Find outliers\n",
    "    outlier_mask = find_outliers(df, column_name)\n",
    "    \n",
    "    # Ensure that outlier_mask is a boolean Series\n",
    "    if not pd.api.types.is_bool_dtype(outlier_mask):\n",
    "        raise TypeError(\"The outlier mask must be a boolean Series.\")\n",
    "    \n",
    "    # Drop rows where the outlier mask is True\n",
    "    df_cleaned = df[~outlier_mask]\n",
    "    return df_cleaned\n",
    "\n",
    "finaldata=drop_outliers(cleandata,'rent')\n",
    "\n",
    "merge_data=drop_outliers(cleandata,'rent')\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "merge_data.to_csv('allcity.csv', index=False)\n",
    "\n",
    "\n",
    "finaldata\n",
    "\n",
    "finaldata.describe()\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "finaldata.to_csv('output.csv', index=False)\n",
    "\n",
    "\n",
    "### 7.Preprocessing Operations\n",
    "\n",
    "finaldata\n",
    "\n",
    "-   This Dataset have city and furnished_type,localityclass  as categorical variable\n",
    "-   we have to transform into the numeric value using Label_Encoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder_city=LabelEncoder()\n",
    "encoder_class=LabelEncoder()\n",
    "encoder_furnished=LabelEncoder()\n",
    "\n",
    "\n",
    "finaldata['city']= encoder_city.fit_transform(finaldata['city'])\n",
    "\n",
    "# Retrieve the mapping of cities to encoded values\n",
    "city_mapping = dict(zip(encoder_city.classes_, encoder_city.transform(encoder_city.classes_)))\n",
    "\n",
    "# Print the mapping\n",
    "print(\"\\nCity to Encoded Value Mapping:\")\n",
    "print(city_mapping)\n",
    "\n",
    "finaldata['furnished_type']=encoder_furnished.fit_transform(finaldata['furnished_type'])\n",
    "\n",
    "# Retrieve the mapping of cities to encoded values\n",
    "furnished_mapping = dict(zip(encoder_furnished.classes_, encoder_furnished.transform(encoder_furnished.classes_)))\n",
    "\n",
    "# Print the mapping\n",
    "print(\"\\nfurnished  to Encoded Value Mapping:\")\n",
    "print(furnished_mapping)\n",
    "\n",
    "finaldata['localityclass']=encoder_class.fit_transform(finaldata['localityclass'])\n",
    "\n",
    "# Retrieve the mapping of cities to encoded values\n",
    "class_mapping = dict(zip(encoder_class.classes_, encoder_class.transform(encoder_class.classes_)))\n",
    "\n",
    "# Print the mapping\n",
    "print(\"\\nlocality to Encoded Value Mapping:\")\n",
    "print(class_mapping)\n",
    "\n",
    "\n",
    "\n",
    "finaldata\n",
    "\n",
    "finaldata\n",
    "\n",
    "finaldata\n",
    "\n",
    "finaldata.corr()\n",
    "\n",
    "model_data= finaldata.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "model_data\n",
    "\n",
    "### 8.Split the data\n",
    "\n",
    "X = model_data.drop(columns=\"rent\")\n",
    "y = model_data.rent.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "### 9.Train The model & model selection\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, model_lr.predict(X_test))\n",
    "r2\n",
    "\n",
    "# get the predicted data\n",
    "y_pred = model_lr.predict(X_test)\n",
    "\n",
    "# getting the true values\n",
    "y_true = y_test\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# mean absolute error (MAE)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mae\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# mean squared error (MSE)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mse\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# R-squared score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "r2\n",
    "\n",
    "#### polynomial Regression \n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# create a encoder object\n",
    "polynomial_features = PolynomialFeatures(degree=5)\n",
    "\n",
    "# preprocess the data to get the polynomials of X\n",
    "x_polynomial = polynomial_features.fit_transform(X_train)\n",
    "x_polynomial\n",
    "\n",
    "### create Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x_polynomial, y_train)\n",
    "\n",
    "# preprocess the data to get the polynomials of X\n",
    "x_polynomial_test = polynomial_features.fit_transform(X_test)\n",
    "x_polynomial_test\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, model.predict(x_polynomial_test))\n",
    "r2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'XGBoost': XGBRegressor(random_state=42),\n",
    "    'CatBoost': CatBoostRegressor(silent=True, random_state=42),\n",
    "    'AdaBoost': AdaBoostRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    \n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Store the results\n",
    "    results[name] = {\n",
    "        'Mean Squared Error': mse,\n",
    "        'R-squared': r2,\n",
    "        'Mean Absolute Error': mae\n",
    "    }\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Model Evaluation Report:\")\n",
    "for name, metrics in results.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "- CatBoost would be the best choice due to its lowest MSE and highest R-squared\n",
    "- CatBoost stands out as the top performer\n",
    "\n",
    "#### Final model for prediction\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = CatBoostRegressor(\n",
    "    iterations=500,          # Number of boosting iterations\n",
    "    depth=6,                 # Depth of each tree\n",
    "    learning_rate=0.1,       # Learning rate\n",
    "    l2_leaf_reg=3,           # L2 regularization coefficient\n",
    "    loss_function='RMSE',    # Loss function\n",
    "    cat_features=[]          # Specify categorical features if any\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2_Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "# Feature importances\n",
    "feature_importances = model.get_feature_importance()\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(importance_df)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, area, type, bathrooms, furnished_type, localityclass, city_encoded):\n",
    "    # Create a DataFrame from the input values\n",
    "    input_data = pd.DataFrame({\n",
    "        'area': [area],\n",
    "        'type': [type],\n",
    "        'bathrooms': [bathrooms],\n",
    "        'furnished_type': [furnished_type],\n",
    "        'localityclass': [localityclass],\n",
    "        'city': [city_encoded]\n",
    "    })\n",
    "    \n",
    "    # Preprocessing steps (these should match the steps used during training)\n",
    "    # For example:\n",
    "    # - Encode categorical variables\n",
    "    # - Scale numerical features\n",
    "    # - Handle missing values\n",
    "    \n",
    "    # Here, I'm assuming some common preprocessing steps; adapt them as needed:\n",
    "    # Example of one-hot encoding (if necessary):\n",
    "    # input_data = pd.get_dummies(input_data, columns=['type', 'furnished_type'])\n",
    "    \n",
    "    # Example of scaling (if necessary):\n",
    "    # from sklearn.preprocessing import StandardScaler\n",
    "    # scaler = StandardScaler()\n",
    "    # input_data[['area', 'bathrooms']] = scaler.fit_transform(input_data[['area', 'bathrooms']])\n",
    "    \n",
    "    # If the model requires specific preprocessing, make sure to apply it here.\n",
    "    \n",
    "    # Predict with the model\n",
    "    prediction = model.predict(input_data)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `trained_model` is your trained model (e.g., a scikit-learn model)\n",
    "# and that you have performed necessary preprocessing steps\n",
    "\n",
    "# trained_model = ...  # Load or define your trained model here\n",
    "\n",
    "# New input values\n",
    "# area = 1200\n",
    "# type = 'Apartment'\n",
    "# bathrooms = 2\n",
    "# furnished_type = 'Furnished'\n",
    "# localityclass = 3\n",
    "# city_encoded = 1\n",
    "\n",
    "# prediction = evaluate_model(trained_model, area, type, bathrooms, furnished_type, localityclass, city_encoded)\n",
    "# print(prediction)\n",
    "\n",
    "\n",
    "evaluate_model(model,1200,1,1,1,1,7)\n",
    "\n",
    "# Save the model to a file\n",
    "with open('model_cat.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "\n",
    "# Save the model to a file\n",
    "with open('model_lr.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2_Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "\n",
    "# Save the model to a file\n",
    "with open('model_dtr.pkl', 'wb') as file:\n",
    "    pickle.dump(regressor, file)\n",
    "\n",
    "# Create and train the RandomForestRegressor model\n",
    "modelrf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "modelrf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = modelrf.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R2_Score: {r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# Save the model to a file\n",
    "with open('modelrf.pkl', 'wb') as file:\n",
    "    pickle.dump(modelrf, file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
